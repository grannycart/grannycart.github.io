---
layout: post
title: Dopecentury VI --- What Makes a Good Operator
category: dopecentury
goat_counter: "dopecentury" 
---

To the untrained eye, the screen appears to be filled with random splashes of color. Sprites that bubble up, grow, explode, and then melt away. Some colors crash into other and meld into a third color. Other colors consume rivals and grow larger, occasionally expanding to the point that they take over nearly the whole screen before weakening and dying away and yielding to another field of newly-expanding sprites.

It took years of practice and training to learn to read what the sprites were indicating with their spectrum-bending color changes. It seemed abstract, but in fact was a way to convey a large amount of information quickly. To the operator that understood the significance of the varying hues and values --- and knew the esoteric key combinations that influenced them --- it was nothing less than observation of entire civilizations, entire worlds, rising and falling away in the space of a few hours session in front of the screen (or screens, as the case may be for the more committed operators).

The software was, at it's core, little more than a simulation. But a simulation so complex and with so many models and variables influencing it that it took years just to learn enough to move beyond the tutorial levels in which the simulator handled most of the adjustments for you. But once enough of the interface had been grasped, an operator might try the more advanced settings in which they had somewhat more control over the simulation. 

This was where things became more rewarding for the operator. With a few keystrokes, they might conjure up a population of ten-thousand, establish a simple economy, new industries, and basic trade with nearby partners. Barring rookies mistakes, the operator might watch their small town grow to a minor metropolis, new technologies develop, and discoveries made. With some guidance from the operator, a large tower might be constructed, or a vast underground fortress. The whole complex system represented on the screen by nothing more than simple shifting pixelated colors, for the designers did not want to waste computational resources on visualizing the simulation, preferring to save cycles for the intensive simulation itself, and keep the visual output to a minimum.

This allowed the simulation to be astonishingly complex. From the interface an operator could call up virtually any statistic that might be imagined and measured: population, race, creed, genders across the spectrum, productivity, economic output, marriages, divorces, true loves discovered, minor lusts satisfied, aesthetic preferences, tendencies towards violence, or noise pollution. You name it, it could be summarized and displayed in a corner of the screen, or if there was a spatial component, mapped out in simple geometric colors, which could then be interacted with.

A more advanced operator might expand their simulation up into a population of millions --- hundreds of millions were not unheard of in elite tiers, with the occasional rumor of someone running a simulation with populations in the billions. The higher the population, the more complexity of course. 

There was a sense among operators that the populations _within_ the simulation themselves had some sense or ability to understand and measure the results of the simulation --- to a point. Once the population crossed into the hundreds of millions, the populations seemed to start losing their grip on their understanding of the mechanics of the simulation. They lost their ability to have any sense of their collective _selves_ because the simulation was simply too complex. It was beyond the capabilities of their understanding. Once in a while an individual among the population might have develop more advanced capabilities and for a short time reveal some new known thing about the simulation to the population within it. Typically that insight would be short-lived, either for a lack of ability to pass on the insight to others in the population, or even if they did manage that, a failure for the insights to keep up with new complexities and the population increased. The population within the simulation were perpetually behind the self-awareness curve. Some operators speculated that there was some limit within the software itself that would prevent the population from ever having a robust understanding of themselves. Some operators speculated that this was indeed a safety feature of the software.

Even most advanced operators could not manage a simulation beyond a population of tens-of-millions. The elite level of operators could push it well up beyond that though, largely through specialized techniques that rolled up complexities with heuristics that simplified the resources to run a more complex, higher-population simulation. So, for example, to push a simulation up into a population of hundreds-of-millions, an operator might start limiting the resources available for individuals to possess unique vehicles or homes, and all the vehicles or homes might start to strongly resemble each other. Vast resources for more complexity in the simulation --- or higher populations --- could be unlocked simply by having most individuals in the simulation live in a home or drive a vehicle that was only a slight variation on a main pattern or two. This, of course, applied to all things the population might interact with, but the most efficiencies were to be gained by putting in place guide-rails on the most complex things that most of the population might be interacting with. So while a power plant might be terrifically complex, and each unique, since only a handful were required the simulator could manage that. While a car is a fairly complex object that nearly every individual in the population might have, so large gains were to be had through simplifying the possible variations on cars the population might drive.

Conflict was inevitable of course, and in some cases beneficial. A growing sub-population of one area or belief system might get into essentially a war with another. Significant portions of population could get wiped out, and one of the colors on the operator's screen might fade away as a result. This could be to an operator's benefit, because it could create an opportunity to expand a new population area with a different set of heuristics guiding it. Conflict was, of course, not the only way to achieve this kind of reset. Similar shifting could be accomplished by unleashing a disease, natural disaster, or virulent tyrant. 

The software was so complex that now set of heuristics ever had the same results twice. As a result, while there were a definitive set of skills that an elite operator had that gave them the ability to grow populations quickly and repeatedly, there were at the same time no guarantees that what worked in a past round of the simulation would work again in precisely the same way. What all operators sought to avoid was stability and stasis. When the simulation got into a cycle that was stable and without change, it was considered akin to death --- it was, in fact, the only way an operator might be said to have "lost" at the simulation.

The goal was always to grow the simulation, to increase the population, to increase the complexity. The creation of complexity was the only real marker that mattered when it came to bragging rights among operators. Though an operator who failed to grow and instead took their simulation down in a blaze of conflict and destruction to a place that would make it easier to rebuild were considered to be pursing a respectable alternative approach.

The only operators who were scorned were those whose achieved some kind of stasis in their simulation, and then carefully and subtly pruned and managed it to maintain the stasis. There was the occasional operator who derived some pleasure from this type of simulator management, but they were considered pitiful --- if not outright insane --- by the other skilled operators.




